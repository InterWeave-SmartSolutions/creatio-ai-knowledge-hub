[
  {
    "chunk_id": "5443f3fd0687e25f56317ba31a680a9b",
    "document_id": "d25ef65c6448",
    "content": "Today is session number nine of our development on creation platform guided learning. And today we will continue to study server side and we will move on with integration tools. So yesterday we finished with our own web service. It was made at creation side was made with the help of C-Sharp sources and we programmed it. We saved it in file system. We used Visual Studio to develop this web service. So we used our examples. Now you know. Now you know how you can make your own integration tools. In general, so integrations will require much more entities and require much more tasks to exchange data. So to be really hard for you to write a web service for each data transfer that you will need in your system. So we will study today how to use standard platform level tools to operate with data with the help of HTTP queries. And we will discuss and I will show you examples of how to work with standard tools and they are all data protocol and data subsurface. Also I plan to show you how to call third party web services with the help of no code tools with the help of settings for the services and call web service item. And at the end of the session, I plan to tell you about Clio 2, Clio Commons. You will understand what is this, how it can be used, why you need some additional tools. So we will discuss a bit more about system maintenance delivery and if you have any questions, I would be really happy to hear and to answer them. So don't be shy. Ask any questions if you feel something that you need to know. And let's move on. So integrations with data tools. First of all, I need to show you general integration capabilities that we have on board. We go to development guides, integrations, options. And here you can see standard options that we have. We already studied custom web service option and it offers us the ability to program anything we need to ask for data to make operations with files to use any libraries. But this approach obviously requires some programming at creation site. And also this approach requires programming at third party site in order to normally correctly call such service and to get response data parts such response. So I just want to say that in general, it's a quite expensive approach if you need to do a lot of different operations. As you can see, we have other options to integrate with creation. We have two options for data transfer such called grad operations, which means create read update and delete. So standard simple operations that you may need to work with your data and to organize possibility to read some data for third party app or to make some inserts updates or deletes in creation by comments from other applications. And we have two different options for this one is called data service. Another is called all data. And all data is quite common because it was designed by Microsoft and it's possible to find all data clients as third party sources. The right out of tools that understand how to work with our data so developers of creation decided to support this type of this type of client and data service is much more unique. It's very specific to creation, but it offers you more options, more possibilities, more complicated calculations. And that's why data services use the full creation client side and data service in general is more powerful than our data and more complex to program. So I will show you both data and data service you will see how it works. Also, it's worth it to mention that we have another integration option to run business processes. So you can use process engine service web service for starting or continue execution of business processes. In the same way how you can do it from creation client side, physically this web service is the only one to handle processes. And when we run processes from inner application that may remind you how we did it. We had the question freedom UI section, we can open it's page and then we made an action to calculate average price with the help of business process. You remember we did this calculation. We started process now we have some results average price in dollars and we save it or close it and that's how we finish our process. So physically we did some calls to third part to process engine web service here can see it. And the same calls with the same parameters can be done with third part application. So it's possible to run processes from 30 third party. And of course, in this case, you need to pass authentication first and we did it yesterday with the help of postman to remember we had the postman authentication. We had set of authentication cookies. Okay, so if interested, you can just watch yesterday video and get more details. So today we'll focus on data transfer integration options. And I would like to start from our data because it's easier. It's more friendly for beginners and it's quite easy to start from scratch. So what it is, this is a data transfer protocol, which is supported at creation. We have all documentation about it. So for all data, we have documentation explaining how to use it. And in general, you need to know that creation supports all data for and all data three. Unfortunately, all data three is not supported in packages. So it's not supported for objects that are saved in packages compiled as separate assembly. So it means that for your all data integration, probably it's better to focus on all data for from the beginning. We have a lot of documentation about it. And if interesting, you can find much more and we have examples. We have references. So I prefer to show you something really, really useful is article. It explains some general basics about how to use all data. We'll try to make some examples. And first, you need to know that all data operates with the help of data model. So it means it respects all existing objects, their columns, their names, and so on. So when you operate with all data, as well as when you operate with data service, it uses data model. So it uses information about existing objects, columns, references, and access rights restrictions. So depending on what operation you need, you have to select a proper HTTP method. So what is used to select post for inserts patch for updates and HTTP delete is used to perform physical delete operation. I will show you a couple of examples. And also you need to know that we have a lot of interesting documentation samples here. And one of the good ones is creation API documentation hosted at document or get postman call. And one of the best sources that you can find for all data. I plan to show you examples with all data for, as you can see, first of all, we have to make authentication correctly. Then we can do different queries and different examples. So depending on your task, you may find the corresponding sample here. And also we have a lot of samples to make different filters. You see for different filters that are special expressions in, let's say, oh, data language and you can find a lot of examples here, even batch query supported batch query means running single query with several parameters in each parameter. Explains to system how to run a particular operation. So it can be useful if your plan is to run many data operations with one single query. So batch operations are also supported, but we can do it. Let's not go too far. And I understand that probably not all of you will start your integration at all. And this is something that you just need to know and say get familiar with it. So let me show you examples. My plan is to make some selection of data. We have some examples of data records that read data from our reality object, but with the help of all data. So I will use integration tool. I will use post on app and my yesterday cookies are not working anymore. They will not help me to run query second check it easily. So we can try to run our web service. And as you can see, we have 401 not authorized. It means that our cookies are related to expired session. So batch cookies will not be able to use any business methods, any business logic methods of creation of services. So we have to get new cookies. I prefer to clear previously set cookies. We go to perform login operation again yesterday. We discussed how to use for use special URL to all service and its login method inside of a root part of our application. We provide login and password, no any special data, no any special headers. You may find some information here like authentication and some accept content type for a new session, but it turns out to be not necessary to perform authentication successfully. So let me show you here. We have some hidden headers made by postman, like accept content type, application, this on and so on, but so I prefer to keep it as is by default. And we have login and password. Originally we have no cookies. So let's try to get new session for us. Yes, we got it 200 and no errors. So currently we have set of cookies enough for us to perform in the clears. Okay, we can create new query and our task, but skip it easy and keep it simple. So we will try to read data. We will have to make a selection of data records and we will make it get HTTP query. Let's do this. So let's go to postman creating you get query. We have to use our address of our app with zero alias here. And then we can look at this example or we can look at this document or get postman comment examples. So we need to do selection and our part for all data with service is all data here. So it's a name of all data endpoint. Then slash in all data for we should use just entity name here. It's not all deals that collection one is entity name and here you can see that so so not so obvious, but maybe you can find examples. Yeah, here you can see an example. So we will make it clear how we can do this. So I will use an example with my usr reality. If I provide no any parameters, system will try to read all the data with all the records or the columns. Let's try to do this and run send for get queries. We do not need to use VPN says ref protection. So for get queries, no cross side request for jury protection needed. And you see took significant time, almost six seconds to run and the as a result, we have some JSON body was different data records about our real G records saved in database. As you can see all of the columns, but a lot of unnecessary data, but that's how it works if we provide no special parameters. Now I will show you how we can make it more interesting and more useful. We can use different parameters in our script, in our query and let me show you, for example, parameters for selecting data collection. And select fields. So here I can see an example. And it means dollar select. So question, dollar select, and then we have columns with comma separated values to get necessary columns. So I'm using this example, you can use this example. And let me show you how we can do. So question mark to switch to parameters. Dover select is parameters specifying column names that we plan to use. Don't forget we have to use column codes, not titles, USR name, USR rise USD. Maybe created on. So if you want to can get more, it's possible to use lookups in order to get corresponding names of type or name of a person who created the record. But now I try to keep it simple. So this is example of how you can run queries. You see only requested columns now here, here. And we can see all the data. We can do some limitation. So we can use and another parameter, dollar top three. For example, and when we do such selection, it will read only top three records for us. You see on the top three records, you can use sort, we can use filter, we can use order by in order to organize some sorting. We can use special parameters to select only one specific record by filter by ID. The load of other options. So if necessary, you will be able to do this. And I try just to keep it simple and quite useful for you. So here is an example of forget query to read some data. So you can try to do it even in your browser. And if you see if you will try to run such query in your browser, you do not need authentication because usually your browser is already authenticated in creation and you keep running session running pages of your creation and your browser. And that's why you don't usually need to prepare separate. To prepare separate authentication for it. Okay, I think for selection, it's quite clear. So let me show you how you can do insert of data. I need to use almost the same set of parameters, but for insert, as you can see here, we have post to add data. Let's do this. We can also look at examples for post. And our URL will be quite simple. We will just provide post and this URL, which includes our website, zero application address, all data is a endpoint for all data service. Then you are realty is also part of an point and it gives information to a data what exact object you want to operate with. Then I have to look at body role Jason and we have to provide some data, for example, you are name is meant to truly data from all data for integration to be our name. USR price USD. So you can see we have some data here if we try to run a post query without says that if settings we will get for all three. Because for post query, we must perform DPM CS RF. HADER we must put corresponding cookie value, please carefully copy cookie value including some dots if they present. In this case, our insert will work. No, it tells that comment doesn't exist. Okay, it was my fault because common name is us are comment. Okay, everything is working now. We can go to our main app, sort by date of creation. And we will immediately see that our data was added. And we can also check column values here. You see price was added, we see column was added correctly, but what we really interested, you can see that type and offer type were set by default. It means our object model worked well for us and moreover, our object model also supported all event handling, provided with help of start signals. Now you see three real to visits were already planned for upcoming days. So all business logic that was designed and programmed to the help of event handling with the help of start signals. It will work for all data as well. And server side handling like we did to validation for very big prices will also work now. Let me show you how it can look. So let's try to add a very big number. And it will be more than one billion. So we will see how system will react on this attempt. Now you see we have a quite good error and this is error 500 in turn of error error, but we have error text, which means we intentionally raised this such exception. We provided error message text of what is wrong and of course physical insert was not done. So we can read data, you see no new records created. So this is example how you can use all data for your tasks and it will be really, really attractive for relatively simple tasks for your future integration needs where you need to transfer. Not so many data records. If your task is to transport millions of rows, possibly such approach with running separate queries for each record will be not very effective, not very efficient. In case if your task is to transport huge amounts of data, you have the only one option. This is your own programming of such data and your own parsing your data structures and your own database direct operations and avoid in use of object model, because when you use object model, you also support all of the object events. It works not so fast and for example, when we did insert with the help of postman, it took almost 180. Now in total, it took a bit longer. You see we have some additional cost for preparation of the query. So it took 300 milliseconds and it's important because if your task is to transport like one million of data. If you spend 300 milliseconds for each one, it means you will wait for several days and this speed probably will be not suitable for your customers. So you'll have to search for a better solution. So in case of a big number of records to process, in case of a very hard speed requirements for your solution, you only see the option is your own service with your own data formats, your own direct database management without use of object model. If in simple real life scenarios, I think all data will work well for you and it's relatively easy because you see it's not so hard to build such query to run it even when your third party app is used. So you can do such programming and also you will get response data in JSON strings so you can easily parse it. You can easily extract corresponding results that you may need. Here you can see JSON objects and you can also get corresponding values from you. So it's very suitable for beginners and for some tasks, I think it will work perfectly. But for some other tasks, you may find that all data is not enough. Maybe because of hard load conditions, you will finally realize that our data is not going perfect with hard load, big amounts of data. Maybe you will need some more complicated scenarios with reading data and performing some calculations like preparing reports and running it with third party app. And you will need some aggregations, some complex filtering in this case, or data probably will be not the perfect solution and data service may work better for it. Data service is created by creation developers, not by Microsoft. And this is unique to creation tools. So it's a proprietary way of operating with data using creation server side. It can do all the same operations like create a read update and delete data. And but it can do it a bit more efficiently and also it supports different filter condition macroses and it's much more rich when you need to select data from different tables, perform some aggregations, perform some complex filtering. I will demonstrate what data service can do. And first of all, I need to tell you that data service is mainly used by inner creation client side pages each time when you operate with any list or edit page, you will see data service web service queries. You will also see that payload for that queues, it means arguments that you need to transport payload is quite complex. You see more than 100 of different settings and he will preview. So if you will view source, you see a lot of settings and parameters that can be passed as arguments for data service, but it also has a response with a JSON. So now you see JSON string, which can be parsed and such JSON data. It's not so hard to operate with. You can easily get corresponding values out from your results. So data service web service, maybe can be suitable in case if for some reasons, oh, data does not work for you as you expect. I will show you examples of data service, but first I wanted to mention that it's used inside of creation pages. So data service was designed for inner creation data operations with client side all lists and all edit pages are using data service service to get or to modify and sort of date any data record. So in documentation. You will find that data service is promoted as integration to. So it's a restful service. You can use our party to compose queries for it. And here it's promoted and advertised like an integration to but in reality, it's not so integration oriented, but it is. Let's say platform UI oriented because all UI pages working with the help of these web servers. I will show you some advanced features that these web servers supports and let me hide some unnecessary columns. So my plan is to show you special types of columns called aggregate columns and also a bit later aggregate filters. So aggregate column can make some calculation over connected data records and such connections will be done with the help of main record. Let me show you what I'm talking about. So each reality record may have number of visits inside. Recently created records are having more visits because we already had automatically creation for them, but also we have. We don't make any action to add the data directly here. Okay, so we have some examples which already have created data records in visits detail. And for each real record, we have number of visits, visits connected to the real records. So we can make a clear column. You can make a aggregate column, which will simply calculate number of connected records. This is one of the easiest possible aggregate columns that we can make. So let me show you how can do this. Add the columns and instead of working with traditional set of columns we have in main object, you can go to related objects. And then we will see all objects that are available for us according to look up columns from main object. Also, we will see reverse joins, we will see other entities that have look up columns pointing to reality. In my case, this is reality visit. We have different options how to select data from it. And in last versions, we have also advanced examples like reading top one record. For example, we can read top one comment sorted by date of creation or we can see top one customer who is specified in recent visits and so on. In my case, I tried to keep it simple. Let's look only at number of records of connected visits. And click select. It's also possible to specify special conditions to select data. For example, we can only calculate numbers number of records that are in the future. Or only visits where contact is specified. So you can do different conditions here. In my case, I also tried to make it easy and calculate total number of visits. And we can make the caption here visits count. So this visits count will be our title for a new created aggregate column. Then we click save. And now you see new column which shows us some data. And for each separate reality, we have some calculation. It's interesting to mention, but this type of calculation is also performed with the help of data service. Moreover, such calculations perform with the same query where a main data record is selected. So if you reload the data using this update refresh button, you see only one query was executed. And if you go deeper in payload, you will see that our column for calculation of aggregates number is also represented at the standard column here. We have some column path for it, some setting for type of aggregation. Aggregation type one means count. And that's how system knows what to calculate and returns as some calculated number. It was example of aggregate column. And it can be really useful and you should understand that this selection is not performed from reality. This column is obtained as a result of sub query from reality visits. And I want you to see one more feature code folders. And here we can make additional folder select and add the new folder here. So in general folders work here like search folders in your outlook. Here we can specify some name and filter conditions that will be useful for us to select only some searching data, not all of the data from our list. Let's call it three plus visits. So let's imagine for some reasons we need to look at real to records, where we have for three and more visits created. We are not interested in real to records with no visits. So we can make such a name here. David, then we can provide filter conditions for this folder. So now I'm planning to show you. So called aggregate filters. We already saw aggregate columns. And you understand that they represent result of subcures, but calculated with the same main period and aggregate filters will be used for selecting data applying conditions on connected records, not on real to data, but on connected records. In my case, I plan to select only real to data, where this visits count is greater or equal to three. So we can make condition. And here we have to select connected entity. So not just dropping down here, contents of real to columns, we have to click on this plus in order to select connected entity. In my case, connected entity is a real to visit. Aggregation type will be just quantity. And also we have alternative options like maximum or minimum date or creation or modification. If we had integer or decimal values there, then it's possible to calculate average price maximum or minimum. In my case, I will just do quantity calculation, select. And we have condition count greater or equal three. That will be our filter condition. We also can make additional filters here like counting only visits in future or counting only visits with not ametic comment. So whatever you think it will be useful, you can do here. Then we save it. And that's how we see result of this filtering. So all data shows us all real records. This search folder shows us all the records which follow corresponding filter conditions. Such filters are called aggregate filters. And we can now go again to network and reward the data to see that our payload also has special filter condition. And this filter condition includes information that we use aggregate function. We use count for our aggregation. So it will do selection only of records where some sub select some count of connected data is greater than certain right expression. And we have just value three here. So my example is to demonstrate that data service, web service is capable not only to read playing data for the data sections, but also it's capable to calculate aggregate columns and to use aggregate filters. And you can use data service for your integration, but I think it's really hard because you will need to have a good make already good to made examples to make it work. So if you make corresponding queries in your browser, then you can steal all necessary parameters. Let me show you how we can do this. We can copy request your all from our query that we spotted in our network. Then we go to postman create new. Actually, you should be careful because data services usually working with post queries only. So we go to postman, regardless of the operation, we will use post query and paste this URL should be careful. Okay, paste your out. We have a huge body. We are going to pay vote, view source select everything copy. Then we go to body here. This is row JSON and we paste this big body. It's really hard to analyze it so we can use beautify tool to see it in a more structured way. So now you see we have a lot of options here that I used by data service and required here to be present. And as you can see, I did not type the menu. I only used the existing example from my browser console. This is a post query. So we obviously will go will fail into CSRF protection if we do not care about it. So the PMCS are had her and corresponding cookie value should get it carefully copy in full value. And based in it here. Now we will successfully run our queue. We have some valid response JSON body and we see set of records. We see set of data. And in general, if you run something like this from third party app, then you'll be able to parse your data and you'll be able to analyze it and get corresponding numbers or other columns if you needed. Other and his asking is folder the only way to go to advanced filter and freedom UI. Yes, currently we don't have any separate advanced filters for data selection. So developer decided to keep it saved into folders in classic UI. We had an option to keep such cold advanced mode and make this filtering like flying in the air without lending anywhere in your system. But I personally think that this kind of advanced filter is not so good because once you take time to build it once you make a complex conditions. There is a highly likely situation, then you will need it to save. So developers and freedom UI decided that no advanced filters anymore. If you need some complex filtering, just mentally prepare yourself to save it as a folder. It's not a big deal, not a trouble and such folders usually have quite strict permission settings. So when you create a folder, only your user will see it. So it will not create too much of garbage records seen by anyone. Only your user will see such records. If you don't like it, you can also remove it. So I think it's not a problem. Thank you, Adrian, for your question. And yes, I agree. We have some some changes between classic and freedom UI. And it looks like such changes were discussed. So it's not a real decrease of some important functionality. Thank you, Adrian. So this example shows that data service is much more capable in comparison with all data because you can use more complicated calculations for problems for filters and everything is going with just a single query. How we did here and demonstrated in postman that it works perfectly. In real life, your integration will include not one or two queries. In real life, I think you will need tens or even more queries to start. So it means that you will have serious of different queries. You will need to remember data and save it somewhere. And in general, it requires some quite strong professional developer skills and architectural understanding of what you are doing. In general, I recommend you to run queries that will not return you millions of records. It's better to operate with data using some portions and both all data and data service have their own limitation. You can find some limitation number of requests unlimited, but integration options. I saw some information about it that we have a limitation about 20,000 of records per one selection. And the same limitation will be applied for data service. This limitation for all data and data service has very similar limitation, but I just suggest and recommend you to operate with data with some smaller portions, keep some logging by yourself. It will be easier for you to detect and understand how actually it goes and do you have any serious errors in your integration. So it's like a programmer task, but it's possible. And in complex projects, we also do this and it works well. So let's move on. We already studied how third party app can operate with old date with creation tools and all data and data service are already present at creation service. So they are already prepared for you. So you only have to program at third party application to correctly call such tools and it will work for you. Now I wanted to show you an example how we can run third party web services from creation and also how to do it with no code tools because no code is really, really attractive. It takes very little time to develop and gives quite quick and valuable results. Let me show you. Let's imagine we have creation app. We have different sections, data and so on. And we have some third party application that we want to call and to use its data to perform some data transfer. Let me show you some example of it. We have a note to its rest API samples. So looking at something like this. Let me show you. This is a rest API URL call that we can send. This is a get query. You can easily do it in your browser. Luckily it does not require any authentication. So it's a get query performing with some kind of URL. No initial arguments, but we have some response Jason body. Jason body is also shown here for us and this Jason body represents some prices. And I need to tell you some physical sense of it. The idea bank is one of the banks in Armenia and they sell gold bars gold plates starting from one gram gold to up to 12 kilos gold big gold slab. And you see the prices are in local Armenian currency, Armenian drums. But in our case, it doesn't matter. So we only want to practice here. I know I want to show you some additional example, not only just get couple of data values from third party app, but also to show you how you can use no code tools to process collections and to get sets of some data records, how to store it, how to operate with it. So let's imagine we have a task to regularly get gold prices from this URL from this API, save it in creation and make it possible to run multiple times and correctly update such prices. And you can use only no code tools for it and you will see how we can do this also you may reproduce it during the session. So let's start we have your own and it's nice if we have some description of this URL. So generally we have it. But in my case, it will be really simple. So I did not really needed, but normally when you work with some kind of integration, you will have this kind of explanation of how to call corresponding query, how to transport some parameters rate go. Okay, now you see example of query, you can see parameters, you can see requests and so on. And in my case, so I can also share with you. This is going to be documentation, but the most important we need example of call, which works for us. Luckily, we don't have any special protection security or authentication necessary for this. You may also find a lot of other sources with similar functionality like currency exchange websites, like weather forecast websites, like other regular date that could be commonly interesting for people. It's usually shared without any special restrictions. In my case, this doesn't require any parameters, any input arguments. So we can do it quite simple. And I think that simple example is better for beginners just to get started. So we have this URL. And then I will explain how we can work with it. First of all, we need to go to studio workplace and find a web services section will for appropriately register our new third party service here. And then we'll be able to call this service from our business processes with the help of call web service item. This will not require actual development skills, but it requires some engineering knowledge and understanding of HTTP queries and HTTP methods type and just a bit of understanding of JSON string. And you will see that it's not very difficult. So let's do this. Add new web service when we created system uses current package system setting. So when we provide an example of a full service URL, it parses it and creates corresponding web service setting and tries to save it. And then we need to take care about the proper package to save. Now you see some phantom packages which we can't really find in our configuration. So let's ignore them. This is classic package because our current package system setting points to it. But my plan is to save our data to 3.0 UI realty package. This your I is like a main part which was extracted from our URL. And it will be used as a like base part of our service and then we can have many different methods method addresses will be added to our main service your address. The code is generated by app. So let's call it something like gold bar service name will be displayed and the code will be used for configuration saving into a package. Finally, we will have a special type of item saved directly to the proper package. So you don't need to care about how such setting will be saved in our configuration. This will be saved as special metadata directly to the package. And we have a method here. So let's save it first. Now you see configuration section and let's go to all items. You will see new type of item code, web service. And it's already saved in our package. So all you can do is to open its metadata. You will see some will rebel text definition of the setting. I can dress us and other settings and so on. But this is a little bit of a setting. You just need to remember that it will be saved in our package. So you don't need to care about special transporting of it. Okay, we continue to do our setup. System was capable to automatically parse our URL and get the service main part and method part. And you can see it's by default. This is a catch methods. Let's look at it. And in general, one web service may have many methods. So you can register them manually in order to use different functionality of the same web service. Get method. The content type is JSON and response time only five seconds by default five thousand milliseconds means five seconds. No authentication necessary. And name is for display code is for metadata to save and method to dress is the most important property here because it represents exact part of the URL which will be used to add and make food. When we do this, we will make full method address in our we have a full method address in our service. And this part was automatically parsed here. You can also type different types of parameters like method parameter method query value. And also it's possible to transport additional data values depending on the type request you use. If you use get query, I can now our case, you only can use method to address parameters inside of this URL. If you're supposed, it's possible to fill in post body request body and it helps to get much more possibilities to transport different data values as arguments to your work service. In my case, I have no request parameters at all, but if you will do it something like yourself with different services, you need to remember that we have all possible ways to transfer data that is common used in the rest API. You can use address parameters, query values, header values. Remember, like we did in our post query, special headers settings. So it's like technical settings that will be passed as part of our query. So creation is also capable to provide headers and even cookies could be provided separately. So if you do some operations with third party system, which requires some authentication or require some cookie for like your user settings, then it's also possible to use it in your query. So it's very universal tool. In my case, I do not need request parameters in this example, but I need response parameters. So let's save, save it again, just not to lose anything. I will use response parameters first automatically. Let me show you. First of all, I can run send a test request, no parameters, no authentication. So I just go and send it and I have a response if Jason or in row HTTP of course Jason looks much more friendly for me and I will just copy this data and this is just an example of what web service answered me. This is my response. Then I go to the method switch to response parameters. So my idea is to tell creation, how should I parse result data in order to extract specific values from it. So in my case, response parameters can be added manually or we can use special very effective tool, which is called quick setup and I will use example of my response body and give it to system in order to detect what are the possibilities, what are the possible values that we can get out from this example. I will use setting of response parameters with the help of example in Jason quick setup example of response in Jason, of course, I have to paste my example of Jason data obtained as a result from my test request. Next system was capable to parse my data. As you can see, it was detected that I have two collections. One is called cash cell, at least of cell prices or bank cells, gold bars and cash by it looks like they have this data, but zero still as that possibly they are not really planning to buy anything. And it's just like usual as part of the data for us. So we can select what part of data is interesting and what part can be can be skipped because in our case, we do not need it. And this is a very good example because in my case, I do not need cash by part, but I need cash cell. Okay, and we can select only part that is necessary. It's important. This is very important because in real life examples, for example, you can call some foreign currency exchange rate service. And you know, we have almost 200 of different world currencies and you may face maybe four or five or even one South end of records. Four or five hundred or so down to records with different values and parameters has response. So it will be really important for you to select only to select only parameters that you really need to get from results of the service because sometimes the web service result bodies quite excessive, including a lot of information that you don't know. But I'm to use or just not useful for you. In my case, I am interested in cash cell list and I will save it. Okay, so this helped me to avoid manual registering of parameters. It's possible. I can create such parameters manually. I can reproduce everything by my hand, but using this quick setup tool by examples is much more efficient. So I have a root item here, which represents a collection is array. And also it's interesting to mention that I created uses such thing called Jason path. Jason path is a kind of address of a value inside of Jason body. So this value helps to detect and get corresponding value out from Jason body text. And creation is using it to allocate and find the corresponding parameters. As you can see, it's body parameter and inside of this array, we have players of data weight and rate. Let's look at our data closely weight usually. So it's shown as a text, but physically it looks like an integer number minimum is one gram and maximum is 12,000 grams, which means 12 kilos. So we can afford to treat weight not as text, but as integer because we see that for all data values here, it will be nice if we have an integer. So we can treat results as integer and system will be okay with it. But when we go to rate and see some examples sometimes. So in general rate is a text and sometimes you see salient separators and sometimes you see it's even twice. So turned out that this mechanism, which gets data automatically with the help of data service works poorly with type conversion. That's why it makes sense to keep rate obtained as text. And if you really need to work with it as with decimal, obviously this is a price, so it should be decimal. So if you really need so you can parse this data lately after you got it from the web service and then later and then you can save it as you wish. In my case, I keep it simple. I tried to not spend too much time on such data type transfer. So I prefer to keep it as text. So it will be just a good for our demo. It will be not so suitable for real life calculations, but wait for it will be not a problem for you to use date type conversion using, for example, you can use script task for it. If really interested, we can try to do this and trying to save our time and to not to not to go quite far from our data obtaining from the service. Okay, so this example takes result data and parses it and finally will present our data as collection of data records. We can save it. And this information is now saved in our configuration section. Now I propose to make a five minutes break. I promise not to go too far. And then we will continue. So let's move on and we finished on the creating of our web service, checking out that our service was saved into our package. This is very important. And now we will use it. So what how we can use it we can create a business process, which will run this call service item will get response data, but we need to think of where we went to save set of records with weight and rate information. And two possible options. One is memory, but it will require some C sharp scripting for us in order to keep data in the radius memory, for example, I prefer to make it more simplified and no code. We will save our data into our database in just a simple look up this look up requires into job weight and text rate. I can easily created our cell in this package. So I will do it and object. This is necessary to organize storage of our data. And this storage will be named us are gold price. Price. No, you can't do it parent object, but not based look up because we don't need name and description. I will use base entity because it will only give us standard. Parent columns and we can add business columns ourselves. So we will create integer us are weight column. It will be integer and one more will be text 50 the minimum one you are a rate will be our rate column text 50 you may ask me why text the answer is because I do demo of getting data in real life. We will need to save our text values first or maybe to use them and their process before saving with the script task and then we will use a decimal value to save finally converted value. In my case, I went just to keep it as demo for you. So we will save a text rate data as we get it from the service. And that's all know anything else. We can publish our object as you remember publish always performs save first. So we don't need to click save button save was performed and publish was done great. But this object already applied one more small step we can go to look ups and register this object as look up. So we click look ups new look up find our gold. And then we have the process save it. So now we have this look up we can open its contents but we have nothing there. We can open properties and create data binding item and save it into our package. This is necessary to remember registering of the object as look up. So instead we want to go to your look ups and find it when we will go and we will deliver our solution to test environment. So now my gold prices object is ready and we can move on to work with processes. We can register it our web service and we can use no code solution with processes to get this data from third party. We can create new process. And we can use it like usr yet. Road price main process. And then we can use main process because I also plan to use sub process to parse my collection. That's why call these processes main process. No initial arguments. Simply starting our process by manual start. And the first step we need to do is call web service process item. And which web service to call we have our ID banking web service. Then which method to call we have the only one method. That's why systems selected it for us automatically. We don't have any request parameters when we switch to advanced mode. We can see response parameters and response is our collection with couple of values here. Also we will get HTTP status code which will be useful to check different errors and analyze what's actually happening. And sometimes full response party will be important because it will be to include everything obtained from third party without any parsing. Boolean success property and probably that's enough. And we also have a request body but it works like this is an input parameter and only for very specific cases. So I don't know how to make a no code example with this response request but it's how we will not use it now. Okay, so when we run this call web service we can run it like get gold rises. This item will finally run this query. Let's go to primary mode get code crisis and it's important. Results will be saved somewhere inside of this item inside of the response parameter collection. So we will obviously have to process this collection somehow. And first of all, I will show you some error handling in case if our request finished successfully. We can turn this flow into conditional flow by clicking on this change type button and then set conditional flow. So we can name it okay and we can check condition this condition will include just Boolean success property if it's true. Then we will go here otherwise we will stop and we will have another terminate item code error. And we can go from our call web service to this terminate item can name it error means something is not good and we can just turn our flow as into default flow. Default flow will be activated if no one from conditional flows worked for us. And default flow will be our error handling. So if we not successfully called our web service, no need to try to perform next steps. We just need to stop our so abort our process and use separate terminate item because it will be stored in our history of execution. So we will easily understand that it finished with an error. Okay, but if everything works well, we need to think of the place where we plan to keep our data. When we first run it, obviously our data table will be empty. So no visible preparation necessary, but when we run it next time our data will be not empty and maybe it makes sense to clean it with the delete data item. So I will do this delete data. And this will be an item to remove data from gold. Rice object. And as you can see, we have kind of protection here. This protection means that for the leads data, we must use some parameter. If we don't, this will not allow us to save roses and commercial errors. So we must use some parameters here. And this is kind of protection from an intentional delete of all the data. But in our case, we intentionally want to do this. So we have to perform some kind of fake theory, which will be always true. And that's how we can make this correct condition to delete data. So I will make a filter like ID is filled in for any existing record. This condition will be true. And that's why I can use it. And it's quite easy and simple to not require too much resources from system to make it. So this will be an item like clear prices storage. So we delete a crisis from our storage. And then we can use. So we have a collection and we have two options to use no code approach with subprocess to parse collection or to use. C sharp code in order to parse our collection with programming. Of course, I prefer no code approach. But save temporary our current progress. Oh, it tells that the changes saved. Cyclic change in package here are key probably it's because of I forgot to make dependencies and settings. Yes, it was my fault. Okay, it wasn't usual, but it was my fault. Well, it's because of current package system setting was pointing to classic right package. I have to switch to realty. Yes, my define my setting and save again. Now I have no cyclic dependencies. Let's go and check our package just a realty package. Should be dependent from dev classic. And the dev classic package should not be dependent from realty. Yes, looks good. Yes, looks good. Very good. So now no cyclic dependencies, no troubles. And this was just because of the package. So that's why. So we worked with process library. If you select the corresponding package. If you start to create your process from here, then your process will be okay with the correct package settings. So possibly we have one more plus for creating processes from configuration section, but not from the process library. Okay, we have first now it's do not repeat this mistake again. And we will select our package and one more package. I need a sub process to parse my collection. The main idea how we can parse collection is to make a sub process and to use arguments as parameters. So we will use parameters and I was a process to accept collection data values. And I will make my process name you as are. Add the gold price sub process. So it will be my sub the process to add the gold prices. I need parameters because the only way to transfer data from main process to sub process is sub process parameters. One parameter will be integer. So I add the parameter which will mean wait. Enter, input which makes it makes it redone only inside of my process, but I don't want to change it. So I only plan to get it as in value. No initial values save it. Another parameter is text 50. The shortest one and it will be rate. And it's also input and no initial value. So here we have a couple of values we expect to get at the beginning of the process. Then we will use our process and our process structure will be really, really simple. We will just use one add data item. We will add data into gold price object. Add price. And we will fill one record and only couple of values were wait and rate will be filled in very easy rate and wait. So rate will take its value from corresponding rate text parameter. Wait. Clicking on this lightning button will get its value from corresponding integer wait parameter. It is not select parameter window always filters available parameters according to the data type. So that's why we see only decimals and previously we saw only text parameters. We have the stuff so you see the sub process is pretty simple and we can save it. Close it. Go back to main process now we will use an item code sub process sub process and I will use this orange item and place it into my diagram. Normally when you want to run a single instance of a sub process you just specify your sub process name. And single instance call means you have to transport their just pair of values. And this means that you will run your sub process only once. But in our case we plan to work with it and parse collection with the help of sub processes. So I will use special settings now please be careful and watch here what I will do now. For example let's go with wait first. So we plan to turn this sub process into a collection processing mode. And this can be done by selecting special values here and such values should be taken from collection. So I click on this lightning button select process parameter and then I will select process elements and our get gold prices call web service it returns collection. And we have corresponding wait column in this collection. So I have to double click it. And that's how our sub process immediately turned into collection processing mode. And it will run as many sub process instances as many collection data records we have. And we have execution most sequential or parallel in my case no need to run parallel sequential means one by one. And we have input collection we are a specified wait column for it now we will set rate column from process parameter rate double click. That's how we do this run it in the background is not necessary here. So we will probably only take some additional performance but in here we can just run the process and we are interested in finishing it fully synchronously no strong reasons to run it in background creating some scheduler executed tasks so we just can run it simply in a current thread. And it runs we can we can also call it like add prices. At prices and it will run at both price sub processes and we'll have as many sub processes as many collection records you found so that's it. Our example is ready so it will be our happy finish item. This is our start. It makes sense to note and make names for all the items it will be like a documentation which makes it easy to understand. And we have current version of it save it. So now we have this process and we can start it from here or from process library. I throw it by clicking on this icon and we have to start main process let's start it. But also before starting it we can enable trace to be important for our debugging and we can also. So that's it. Okay, trace is the only one option suitable for us so let's run this process. Then we go to process long. We will see our main process started and it took a bit longer than 1.2 seconds. And then you see number of subordinates uprocess instances execute quite fast but we have approximately 11 items here. So we can check main process and open its execution diagram. So we can see how many times our sub process started 11 starts no errors. And it looks like everything went fully okay. You can also see trace data. We are interested in how exactly our gold prices were obtained so we can click on show trace data. You will see all the parameters in your collection. We can see technical parameters like response status code 200 means everything is okay. Full response body sometimes can be important. And it looks like in our example everything went smoothly about any errors. So this trace takes some additional performance but in general it's a good idea. And this trace helps you to understand if something goes wrong and you will see exact parameter values, exact status code. So I recommend you to keep trace on if you want to support possibility of quick quick discovery of something if something goes wrong in your integration. So in my case everything went well and we go to lookups to see exact prices some gold prices section. So lookup list and we can just display weight. Save it. Also it makes sense to look at the columns of creation date of creation. Okay and now you see we have all obtained data saved in creation physically this is text. So if really needed we can use parsing we can use some start scripting to parse such values into decimal and then save them into corresponding decimal fields but technical side of running queries and asking third party system to return some data. I think it's quite clear in case if you have any questions feel free to ask I will be happy to answer this is example how we can call third party apps and at the beginning of course you will need some example of a call. So some URL example of parameters maybe some description and documentation how to call this public API and you have all steps made with no code and finally we have a process which takes probably less than hour to develop with all the explanations and we successfully got our data if we start this process one more. One more time you will see in our lookups you will see new data so you see date in time was changed so our contents of previous left hand records successfully removed and we have new data records inserted just seconds ago. This is also important when you develop your integration to check how it runs multiple times because if it runs once it's okay but you should expect it to run regularly and it should properly include existing data and correctly operate with it probably removing it probably updating it it's up to you in case if you have any questions about it time or place. My next topic for today is to explain your how to use clear tool and explain why you needed so first of all I should explain why we have some additional tools and not inside of the base platform so previously we already had a tool that was necessary to support developers and this tool was called. Let me show you the library tools and workspace console overview so previously we had a special tool called workspace console and it's probably more than 12 years old and it was used only for developers and only for developer tasks and times tasks were included based product preparation processing. Processing resources translations and so on and finally it evolved into complex tool with many more than 50 different functions related to C-shap sources related to store localization files six smells working with file system working with Russian control. And so it has a lot of different let's say quite technical purpose which probably will be not necessary for end users so developers used it for one time but now this is an old tool which has its own disadvantages and developers wanted to make something new so they made a new tool called it clear you can find it in GitHub ATF. Clear if you search for this you will easily find GitHub repository. You can find it's a root folder and then you can see a list of own owners or maybe authors or contributors into this tool and you need to remember that most of them are creation employees on the some of them are outside of the company. So you see some strange users here and there are some people from outside of creation but most of them are from creation so generally you can consider as this tool was written by creation in general. But this tool has open source code you can easily not easily but you can analyze its source code you can. Like propose your own improvements into it you can even add your own function and also collaborate and helps to fix maybe some bugs and maybe to improve the implementation and so on. So this tool is open source so it's free of charge and you can use it at your own this tool has also more than 50 different columns comments and I need to quickly show you how you can use it and why you may need it. I will focus on the most useful examples that you may face when you develop your projects but you should keep in mind that this tool was written by developers of base product so they had their own reasons and needs to operate inside of base product and now you will see this difference so let's go and I will show you how to use it. This tool was designed to use by command line so you should use some windows or if you use other these other operating system you should use command line for it. And first of all we need to check presence of this tool and you can find documentation here how to install it. So let me show you must install dot net core framework on your PC if you want to use clear already have it dot net tool list. This is common showing me existing installed dot net core tools on my PC and clear is already installed here OK I can remove it dot net tool uninstall clear G it means remove it from my system now if we check we have no creation clear tool on my PC. This is a common line utility it's managed by common line parameters and the most important property of it it operates with target creation system with the help of the service calls so clear is managing creation by help of the services it means that it can manage local or remote instance. So it's the same well so unlikely to work space console which required file system access and database access to your creation clear requires only internet network access to your creation OK let's install it dot net sorry to install clear G this is common that you can type in your windows. A common terminal this is tool this is common to install clear it will install the latest version let's see what version we will have now of course this comment requires internet connection so it goes to new get searches for the last package which it can find and now you see the last version is 616. So this is current latest version of clear OK we installed clear but not set it up fully we already can run clear command to see 20 or sorry my fault 20 of functions that it can do for you so let me show you. Where is my scroll bar here you see more than 60 different functions and I have I have suspect that the most early created functions are at the top of this list and the most recently lately created functions are at the end of this list you will not need all of them. So it's like a universal tool it has a lot of different parameters a lot of different. Common so I will show you the most important the most practical that you may need and after installation we need to tell clear what environments we will work with because clear operates with target website and it needs to know your own login and password so we can use command clear show the bad place. And it will show a list of registered applications that are already present on my disk and physically saved into these settings Jason file located somewhere in my user folder on disk you see my previous attempts here so I already have some previous environments registered I have to register my current environment clear. I need to show you web app of course you need to know how to spell certain comments how to get the parameters so you can try to do something like this and for some comments on doesn't show us okay let's do this like this. So for most of comments you can type. Minus question and you will see some error that it did not. I did not understand the comment and it will show you a list of all supported comments most of comments have their short version and most of parameters have their short version so in order to save your space you can use shorter parameters and the red the bad command is used to register your app so it will be included into this file and then you will refer to it by name this is easier than providing your login and password each time when you need to operate with target environment so let me register my current environment clear. I already have some example so let me type it so I have D1 to do yes this is my environment okay L means login supervisor P means password supervisor and my name will be D1 so this is how I do my register. Copy it for you so you will remember it and you will understand why I needed so when I register my app we have some additional travels and both environment places network so we have some travels in exist and app settings JSON okay it was not expected okay show the bad list wow we have some travels in this file line 41 probably because I have some older versions of such settings okay let's go where it tells us users what else users or user update local creation my user update local creation sorry sorry sorry local creation your app settings JSON possibly I have some incorrect settings here or maybe outdated settings all I have something like this which obviously doesn't work good to a good and probably it's a result of some previous stuff it looks like I have some incorrect settings here so okay environments closed then here it's end of environments here is end of all stuff also I could remove my previous items here so I have no environments now let's try to ask it clear show the bad list no troubles you see everything is okay clear read the bad configured correctly okay great so now we can do some simple operations with it clear ping and then we have different options but the easiest way is to use e key for environment and then environment name so this clear ping will physically check availability of our service and if it's okay you will see yellow sorry not yellow green color checks or ping was successful one of the functions that can be useful is restart so you can use restart function restart command clear restart e d1 this is for restart of our app you remember we used maintenance tools add on for this but also you can use this command to perform restart usually it's safe and quite quick you will see what is happening now our application has been restarted it takes up to 10 seconds and sometimes you will need this restart as part of your development process you remember when we modified some objects added new columns we had to restart our app to properly apply such changes so we had such scenarios in our in our training when we needed it and simple log out and log in did not help okay so let's move on we already started to start we have a lot of other comments and clear and you will see plenty of them a lot of comments I will show you the most important ones so from project development point of view you need to know that clear can really help you with saving packages and loading packages so you can use clear to download packages this one this command download packages from source environment for example it can be developer environment and then you can use install package let me find it so we have download and we should have upload push pick a G let me find it should be somewhere here yeah push pick a G it has also short command install I don't know why developers did not show it here install command command is capable so it should be shown here so you see the commentation probably is not perfect and the install command loads our package to target environment this can help us to organize CI CD continuous integration and continues delivery with a single script that can be started with a one step one operation we can do download and then we can install it but also you need to know that create clear has more more other columns sorry comments and some comments like we use for download or install are using standard base product creation of services so system that we work with doesn't require any special setting but if you want to use all power of existing clear comments you must perform special command and this command is named install gate this command downloads from you get and installs in your target system special package which keeps DLL inside and this DLL includes all necessary web services that enable this set of commands so such services simply speaking parts of clear to implement corresponding comments without this comment without install gate without additional package installed to your system clear will be not capable to perform corresponding comments it will show you error 404 which means missing functionality in your target environment so clear only runs web services at your creation and if it fails to run the service that it needs to perform a corresponding comment you will see this error so let's try let's try to run a command to install gate clear install gate ED1 this is required in case if you want to enable full power of clear comments so in my case I will do it in my source environment this takes time because it downloads package from nugget it installs this package into your target environment this package is called clear gate so you will see new package clear gate in your system and this package will have so called file contents and file contents will include compiled class classes for your web services necessary for clear to work but let me show you so here we have configuration section okay it looks like we have some some stuff here let's reward the package, reward configuration section we have some interesting information here I did not expect it it looks like we have some troubles with old data for compilation so system tried to compile and failed exceeded narrow tried count work a process so our IIS prevented our system to compile let me check did I detach of course my visual studio was detached okay I need to look at such questions and also we see some troubles with old data data and probably it's file system will show okay so now system tried to install and compile package named clear gate I should expect to see here no now sort was performed with this case in sensitive sort clear gate is the package which functions may be necessary to run clear comments and you see only couple data items for courses not full stuff so we go to file system go to our application go to our packages clear gate package you see it size five megabyte of executables not so little files been and here here here here and here are DLLs included as file content for this clear gate package so such DLLs are used to execute clear comments they implement the implement web services in those courses into creation server site so that's actually how clear may work on a certain environment and how clear may create and implement new comments because if you want to run something your target system must be able to do this okay so if necessary we can fight with it looks like I have not so much time to date to fix this compilation issue my command to install clear gate package was quite correct so it looks like everything is okay so now I can show you some examples how you can use clear efficient efficiently for project development and this will be comments to save and the world environment settings and sorry save and world packages let me show you I already have another test environment you probably forgot about it so we have D1 Studio as development environment and the second was used as a target and the test environment let's check it out it should be alive so let's go D2 it used as test environment so why it's loaded I can also use clear to register to register a new environment so we will use it here and then we will use it to clear drag the map I need to register D2 and I will call it D2 test to make it clear that it will be my test environment ok register it clear show the bad list my D2 test and clear ping E2 test so I will check availability of my second environment and it looks like should be ok yes looks like ping is ok here is my second environment so my plan is to show you the most useful usage of clear for project development this is CICD automation for saving and loading of packages I already have examples of scripts that perform this save and load so let's take this one probably clear download so we can download set of packages let's check out how many source packages we have so we have dev classic then we have us are realty and also we have us are a realty migration so we have three packages here us are realty migration and source environment is D1 so clear download is the same as clear pool pkg command and we can download set of packages not only one but several packages from source environment destination path is somewhere like this get it dev ok we did the May 2024 so we will finally have a result of a zip file including all our packages we can use this zip file into next command which will be used for loading clear install the same as clear pool push pkg we will use source file we will use on target environment D2 test and we need to use logs because it's important to see some technical details so I will share such comments for save and load with you so it will be really important for you if you want to practice with it and now let's let's check out how it works we will remove unnecessary examples and this will be my save and load example of CIICD let's try first system will save three packages into gz files organize into one single zip file and then it will be done and as you can see it makes it as a one operation if you do save manually saving of an app or saving of a separate package so you will have to perform it with separate steps as you can see it went but not so fully correctly I will remove unnecessary files from my example it's 7 apriol so here is our saved zip file so it looks like save part went okay and the main part looks like I had a couple of exceptions here so you see we have a location package with more file descriptor more than one file app descriptor gson what it is what package it's about real to migration no I don't know so something wrong with application descriptors we can check it at our file system probably it's a result of something messing with package dependencies so our realty has its own information of app descriptor yes this is realty app okay realty migration has its own app descriptor yes and our dev classic should have no dependencies from existing realty stuff and it should have all we have a descriptor extension something strange here and it looks like probably system failed to work with it but it looks like it's not a big trouble and you see one more issue here parents schema and was not found yes this is more serious trouble because it looks like our target environment was not fully prepared to install our changes because it missed customer 360 app possibly previous error message that we had here previous message with this stuff here probably it was also missing corresponding app and we used so our solution expects that target system will have customer 360 in my example customers current our target environment did not have customers 360 that's why we have this stuff so in order to fix it of course we have to install customers 360 first and then our solution next in this case it will be loaded fully correctly but now we can test our target environment so usually we need to reload it we can find our items like oh we don't have any probably it did not like it okay we have to install customer 360 here and then load our solution so you see only the classic was loaded here and recent stuff was not loaded okay let's try to install customers 360 install continue I did not enable file system development mode here because it's like a test environment and no any creation no any external ID so it's installed I install customer 360 then I can try to load my settings again that's how we will see our working examples so I will add when to show you fully correct transition of my environments this is customer 360 installation on my target system as you can see previous results of manual load of package manual load can be done with new application and then install from file now we have successful installation great if you're the lot we will see customer 360 up added yes customer 360 was added now we can try to install our stuff one more time here let's remove previous package and one more thing before we continue so this is not so obvious but when we developed C sharp sources we used external editors we have C sharp code saved on disk only and in order to make everything correct we need to download all items that we developed manually inside of our embedded editors back to disk and then we have to upload changes from disk this is very important because our database part of C sharp sources may have all the C sharp source and if we ever try to compile our package on target environment we may have very unpleasant situation when we will try to compile old contents of C sharp sources instead of actual one so in order to make correct export of your solution to test and production we need to save everything on disk then we have to take everything from disk update packages from file system now you will see system will show us differences in C sharp code because both examples were written here and here were written with the help of external editors this is very important now our system is ready to export so let's do this again save and load thank you Adrien for your time yes we are almost over we will continue tomorrow with preparation for developer axon and also answer your questions it will be not so hard and thank you for a time so now I just want to make it correctly with save and load and then if everything goes well we will see our apps we will see our functionality loaded to our target environment if something goes not as planned this is also good because we will see some exceptions you will see how we can travel shoot them it's important to understand how you may do some fixes when your package is not installed as you plan now you can see do we have any errors yes we had some errors here and it's interesting because it should not be so we should not have any issues here possibly it's about data let me check here we have a log file and we have data insert issue so C-smodule entity usr reality no then this module in workplace studio so violation of primary key it's because of data binding which does not keep correct ideas but duplicate keys is module in workplace so it's registered in a section in a workplace and this duplicate is possible because of the way how data binding saves it so we have one data item failed to load but in general okay one more data item failed to load so we have only troubles in couple of data items but in general data was loaded so we have as a result some errors but this is because of something is not exactly the same as our original stuff okay let's try to reward our section let's see do we have anything loaded studio my applications we have real to section we have classic we miss real to sections here because such data items were not loaded I will show you how you can fix it let's go to advanced settings let's find our new packages loaded here so we can search for real team a realty package and we can see couple of data items so let's see only status has error or needs to actualize here here so you can see list of items that were failed to install couple of escrow scripts from base product we don't care and only one item in our realty package was not loaded we can see error message text and properties so violation of primary key it looks like system can't find the correct value of what it's hard to see okay let's see so it can't find correct duplicate value of this module in workplace it looks like we already have this module in workplace ID and it looks like this data entity was not good so we have some items here and we have key which was not ID and key was used by section and workplace so we can fix this it's not good and this is behavior made by data binding tool I don't really like it so we can fix it with the another data item organizing key by ID that will prevent the data from inserting twice with the same idea so we can fix it let me show you this is module in workplace and studio so it's our realty section this is a date data item this module in workplace for studio and such settings for key is not good ID key is better position for update if in case a record was found great and we can save it and we have a lot of warnings but it's about existing date so it's actual as it's save it just a lot of warnings about reorganizing of the same existence sections that already present in data and also one more thing which we potentially have the same troubles is module in workplace for this realty and it's 26 my application also it makes sense to our ID is already set here okay great probably it can't put position for updates so let's data save it great everything is okay so we fixed it we have to save everything on file system just to make sure we will correctly have all the data well in our repository version control so now you will see that it's possible to save and vote packages again we can prepare something like older like v1 plus old file now we will have a bit better version of it okay saved now let's try to run this transport again so it will be final one and this is example how you can automate delivery of your changes how you can take data from the developer environment and automatically save and the vote it to some other task target test environment of course as developer you understand that sometimes things are not going very smoothly sometimes we have unexpected errors and having such errors during the training is also good because it helps us to see how you can troubleshoot how you should search for error details and how you should do all changes so now you see next time we loaded it took significantly less time because water is smart water text takes zip file takes information about its items all the packages all the items of configuration and it analyzes date of modification if date of modification of the loaded item is the same as date of modification of items that you have already in your target environment then it just keeps it and that's why if you have a big big solution but you have tiny small changes there from recent upload that you did previously it will only analyze changed items when loaded on target environment and final total installation time will be small and now you see only couple of data items that were failed were now updated and fully applied and everything else was correct so now our system in our target environment has no errors as a result of installation we can go to all packages to see items in the actual only base product stuff okay don't care no errors okay so our package with reality was fully correctly loaded all the items that we have were loaded well and we can test it at our user interface so you can go to corresponding sections you can try to create data records you see all three actions all three sections here so our reality with columns you can create new data so finally cat here and you see now we have default values working we have some test data test price negative price validation works two big price will also work and system will not allow us to save because of server site okay like this but now you see prices more than one billion so everything is working as expected so that's how you can deliver your solution to test and check it out and perform corresponding full scale test for your system to check out works now you see details were filled in so everything works quite well and that's how you can do some automation is clear clear has also a lot of interesting other comments I will show you just one of them clear SQL select name for contact okay something like this so this is example for clear SQL execution sometimes when you work in cloud with cloud environments you just can't connect to the database directly and operate with it so sometimes you need to run some simple select queries in database to make sure you have specific data and that's how you can do it with your clear so this is example of how you can read list of contacts from your database of course your queries could be much more complicated and when you run SQL purists please be extremely careful because SQL SQL statements is the way how you can easily damage your system how you can easily destroy your database or you should be extremely careful and please check your queries before execution. SQL can really help you and to see some data directly unfortunately creation has no tools to work with SQL easily from user interface now previously we had such too but was designed by some Russian partner and as you remember you know creation give ups any any work with Russian or be Russian partners and customers so we also removed all the questions that are add-ons from the marketplace and that's why now we don't have any fancy good looking tool to run SQL query from application user interface but now this is something that you can do and the last information that you need to know about Clio is that it's also possible to use add-on for visual studio named Clio Explorer Clio Explorer is an add-on that you can use in visual studio code it automatically loads your information from file with connections for existing solutions and you can use a lot of Clio commands from user interface including SQL. So you can do something like this then you can run your query let me see where there is a button to run query this one so this is Clio SQL comment and it tries to run my query but what? Oh D1 I already connected probably connected yes what's wrong there split editor more actions oh open this yes it was case sensitive I don't know why it's case sensitive but now you see some data we got from the database and you have some kind of UI stuff but with the help of Clio Explorer Clio Explorer uses installed Clio tool so without Clio installed it will not work but this is user interface feature which can help you to work with a particular environment. So we have a lot of functions of Clio the most commonly used and some of them and also you can use Clio commands from terminal window of visual studio the same as we used in other windows console console bars. So thank you for your time today. Today our session is over we will prepare for the exam tomorrow as usual you will receive video recording of today session if you have any questions ask today or prepare questions for tomorrow because we will have more time tomorrow thank you very much for those who stayed for this moment you will see videos soon thank you and goodbye. I did me tree it's boss here yes pass please and tomorrow I'm I have other obligations to attend to so I will probably follow it by video but is there any special action I have to take to take the fast track on development certification. Yeah I will explain so I will send you final email with all video recording with homework for your fast track certification and quick explanation so what you should do so if you agree to run fast track certification you should just respond to my email and we will arrange individual date and time for you to run your exam your exam will include check of the homework so you should prepare homework. Before exam starts you will have a couple of weeks to do this and you should prepare for the test using self assessment tests at Academy I will show and so at the exam will look at your homework and you will run your online exam test. If you fail you can run this test again later so don't worry it's a bit nervous and understand and fast track certification is free for our guided learning participants. Okay cool so you will receive all necessary stuff and videos explaining how to prepare for the test how you should answer questions and so on you will see the type of the homework. Racky is asking will include so let me quickly show and went to show it tomorrow but okay you ask now so let's do this I will quickly show you so the type of the homework will include making your section. This one making your detail make a program the validation some calculations the service and for those who want to run an advanced level some additional business process and the adding data records and some automatic update you can use live update to automatically refresh your screen or I will show you how you can use WebSocket messaging if you prefer to do it a bit more professionally. So the homework simply is just similar to what we did during our sessions and it requires some programming for validation calculations and the charts so it requires some JavaScript and the C chart scripting and so it will not take too much time for you I think and I hope that it will be clear so we discussed all the steps. How to do this if you will have more questions you may ask tomorrow so Racky I think that those who passed our sessions watched our videos and practiced with their own you can use even your own example because the name of the section is the same set of columns very similar so you can use your training session environment to perform your homework on it. So it may be helpful you do not need to create a separate environment for your homework. Okay, thank you for your time today sorry for a stay in a bit later than usual and if you will have more questions so prepare for tomorrow we will have time for this. Thank you very much for the day session and goodbye. You will receive homework assignment tomorrow as well as all video recordings and all the questions that will answer tomorrow. Thank you and goodbye.",
    "chunk_type": "paragraph",
    "chunk_index": 0,
    "metadata": {
      "source_type": "video_transcript",
      "video_file": "Recording12.mp4",
      "duration": 8297.2
    },
    "word_count": 16631,
    "token_count": 18171,
    "context": {
      "heading": null,
      "paragraph_count": 1,
      "position_in_document": 0
    }
  }
]