name: Test Pipeline

on:
  push:
    branches: [ main, develop, 'feature/*' ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'

jobs:
  # Python unit tests
  python-unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Run Python unit tests
      run: |
        python -m pytest tests/unit/ -v \
          --cov=. \
          --cov-report=xml \
          --cov-report=html \
          --junit-xml=test-results/python-unit.xml
    
    - name: Upload Python coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: python-unit
        name: python-unit-${{ matrix.python-version }}
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: python-unit-test-results-${{ matrix.python-version }}
        path: |
          test-results/
          htmlcov/

  # JavaScript/TypeScript unit tests
  js-unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        node-version: ['18', '20']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Run TypeScript checks
      run: npm run type-check
    
    - name: Run linting
      run: npm run lint:check
    
    - name: Run unit tests
      run: npm run test:unit -- --coverage --watchAll=false
    
    - name: Upload JS coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage/lcov.info
        flags: javascript-unit
        name: js-unit-${{ matrix.node-version }}
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: js-unit-test-results-${{ matrix.node-version }}
        path: |
          coverage/
          test-results/

  # Integration tests
  integration-tests:
    runs-on: ubuntu-latest
    needs: [python-unit-tests, js-unit-tests]
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Install Node.js dependencies
      run: npm ci
    
    - name: Generate test data
      run: |
        python tests/generators/test_data_generator.py
        npm run ts-node tests/generators/test-data-generator.ts
    
    - name: Run Python integration tests
      env:
        REDIS_URL: redis://localhost:6379/1
        DATABASE_URL: sqlite:///test_integration.db
      run: |
        python -m pytest tests/integration/ -v \
          --junit-xml=test-results/python-integration.xml
    
    - name: Run JavaScript integration tests
      env:
        REDIS_URL: redis://localhost:6379/1
        TEST_DATABASE_URL: sqlite:///test_integration.db
      run: npm run test:integration -- --watchAll=false
    
    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: test-results/

  # End-to-end tests
  e2e-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        npm ci
    
    - name: Install Playwright browsers
      run: npx playwright install --with-deps
    
    - name: Start application
      run: |
        python -m uvicorn ai_knowledge_hub.enhanced_mcp_server:app --port 3000 &
        sleep 10
      env:
        REDIS_URL: redis://localhost:6379
        DATABASE_URL: sqlite:///e2e_test.db
    
    - name: Run Playwright tests
      run: npx playwright test
      env:
        TEST_BASE_URL: http://localhost:3000
    
    - name: Upload Playwright report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: playwright-report
        path: playwright-report/

  # Performance tests
  performance-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[perf]')
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Generate performance test data
      run: python tests/generators/test_data_generator.py
    
    - name: Start application
      run: |
        python -m uvicorn ai_knowledge_hub.enhanced_mcp_server:app --port 3000 &
        sleep 10
      env:
        REDIS_URL: redis://localhost:6379
        DATABASE_URL: sqlite:///perf_test.db
    
    - name: Run Locust performance tests
      run: |
        locust -f tests/performance/locustfile.py \
          --host=http://localhost:3000 \
          --users=10 \
          --spawn-rate=2 \
          --run-time=60s \
          --headless \
          --html=performance-report.html
    
    - name: Upload performance report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-report
        path: performance-report.html

  # Security tests
  security-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety
    
    - name: Run Bandit security analysis
      run: |
        bandit -r . -f json -o bandit-report.json || true
        bandit -r . -f txt
    
    - name: Run Safety check
      run: safety check --json --output safety-report.json || true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  # Test summary
  test-summary:
    runs-on: ubuntu-latest
    needs: [python-unit-tests, js-unit-tests, integration-tests, e2e-tests]
    if: always()
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v3
    
    - name: Generate test summary
      run: |
        echo "# Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Test Status" >> $GITHUB_STEP_SUMMARY
        echo "- Python Unit Tests: ${{ needs.python-unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- JavaScript Unit Tests: ${{ needs.js-unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Integration Tests: ${{ needs.integration-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- E2E Tests: ${{ needs.e2e-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Coverage Information" >> $GITHUB_STEP_SUMMARY
        echo "Coverage reports have been uploaded to Codecov." >> $GITHUB_STEP_SUMMARY
